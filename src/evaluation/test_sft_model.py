"""
SFT æ¨¡å‹æ•ˆæœæµ‹è¯•è„šæœ¬
ç”¨æ¨¡æ‹Ÿçš„é¡¹ç›®ç»“æ„æµ‹è¯•æ¨¡å‹æ˜¯å¦èƒ½æ­£ç¡®è¾“å‡º tool_calls
"""

import os
import json
import torch
from datetime import datetime
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# æ¨¡æ‹Ÿé¡¹ç›®ç»“æ„ - ç”¨äºæ„é€ çœŸå®çš„æµ‹è¯•åœºæ™¯
# æ ¼å¼ä¸ç”¨æˆ·å®é™…çœ‹åˆ°çš„æ–‡ä»¶æ ‘ä¸€è‡´
MOCK_PROJECTS = [
    {
        "name": "flask_blog",
        "description": "Flask åšå®¢é¡¹ç›®",
        "structure": """flask_blog/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ user.py
â”‚   â”‚   â”œâ”€â”€ post.py
â”‚   â”‚   â””â”€â”€ comment.py
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ auth.py
â”‚   â”‚   â”œâ”€â”€ blog.py
â”‚   â”‚   â””â”€â”€ api.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ email.py
â”‚   â”‚   â””â”€â”€ validators.py
â”‚   â””â”€â”€ templates/
â”‚       â”œâ”€â”€ base.html
â”‚       â””â”€â”€ [+12 files (12 html) & 2 dirs]
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_auth.py
â”‚   â”œâ”€â”€ test_blog.py
â”‚   â””â”€â”€ conftest.py
â”œâ”€â”€ migrations/
â”‚   â””â”€â”€ versions/
â”œâ”€â”€ config.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ run.py""",
        "queries": [
            "Find the user authentication function",
            "Where is the login route defined?",
            "Find email sending functionality",
        ]
    },
    {
        "name": "ecommerce_api",
        "description": "Spring Boot ç”µå•†åç«¯",
        "structure": """ecommerce-api/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main/
â”‚   â”‚   â”œâ”€â”€ java/
â”‚   â”‚   â”‚   â””â”€â”€ com/
â”‚   â”‚   â”‚       â””â”€â”€ shop/
â”‚   â”‚   â”‚           â”œâ”€â”€ controller/
â”‚   â”‚   â”‚           â”‚   â”œâ”€â”€ ProductController.java
â”‚   â”‚   â”‚           â”‚   â”œâ”€â”€ OrderController.java
â”‚   â”‚   â”‚           â”‚   â”œâ”€â”€ UserController.java
â”‚   â”‚   â”‚           â”‚   â””â”€â”€ PaymentController.java
â”‚   â”‚   â”‚           â”œâ”€â”€ service/
â”‚   â”‚   â”‚           â”‚   â”œâ”€â”€ PaymentService.java
â”‚   â”‚   â”‚           â”‚   â”œâ”€â”€ OrderService.java
â”‚   â”‚   â”‚           â”‚   â”œâ”€â”€ InventoryService.java
â”‚   â”‚   â”‚           â”‚   â””â”€â”€ impl/
â”‚   â”‚   â”‚           â”‚       â””â”€â”€ [+4 files (4 java) & 0 dirs]
â”‚   â”‚   â”‚           â”œâ”€â”€ repository/
â”‚   â”‚   â”‚           â”‚   â””â”€â”€ [+5 files (5 java) & 0 dirs]
â”‚   â”‚   â”‚           â”œâ”€â”€ model/
â”‚   â”‚   â”‚           â”‚   â”œâ”€â”€ Product.java
â”‚   â”‚   â”‚           â”‚   â”œâ”€â”€ Order.java
â”‚   â”‚   â”‚           â”‚   â””â”€â”€ User.java
â”‚   â”‚   â”‚           â”œâ”€â”€ config/
â”‚   â”‚   â”‚           â”‚   â”œâ”€â”€ SecurityConfig.java
â”‚   â”‚   â”‚           â”‚   â””â”€â”€ JwtConfig.java
â”‚   â”‚   â”‚           â””â”€â”€ util/
â”‚   â”‚   â”‚               â”œâ”€â”€ JwtUtils.java
â”‚   â”‚   â”‚               â””â”€â”€ EncryptionUtils.java
â”‚   â”‚   â””â”€â”€ resources/
â”‚   â”‚       â”œâ”€â”€ application.yml
â”‚   â”‚       â””â”€â”€ application-dev.yml
â”‚   â””â”€â”€ test/
â”‚       â””â”€â”€ java/
â”œâ”€â”€ pom.xml
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md""",
        "queries": [
            "Find the payment processing logic",
            "Where is JWT token generation?",
            "Find order creation endpoint",
        ]
    },
    {
        "name": "react_dashboard",
        "description": "React + TypeScript ç®¡ç†åå°",
        "structure": """admin-dashboard/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ common/
â”‚   â”‚   â”‚   â”œâ”€â”€ Button.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ Modal.tsx
â”‚   â”‚   â”‚   â””â”€â”€ Table.tsx
â”‚   â”‚   â”œâ”€â”€ layout/
â”‚   â”‚   â”‚   â”œâ”€â”€ Header.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ Sidebar.tsx
â”‚   â”‚   â”‚   â””â”€â”€ Footer.tsx
â”‚   â”‚   â””â”€â”€ charts/
â”‚   â”‚       â”œâ”€â”€ LineChart.tsx
â”‚   â”‚       â”œâ”€â”€ BarChart.tsx
â”‚   â”‚       â””â”€â”€ PieChart.tsx
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ Dashboard/
â”‚   â”‚   â”‚   â”œâ”€â”€ index.tsx
â”‚   â”‚   â”‚   â””â”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ Users/
â”‚   â”‚   â”‚   â”œâ”€â”€ index.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ UserList.tsx
â”‚   â”‚   â”‚   â””â”€â”€ UserDetail.tsx
â”‚   â”‚   â””â”€â”€ Settings/
â”‚   â”‚       â””â”€â”€ index.tsx
â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”œâ”€â”€ useAuth.ts
â”‚   â”‚   â”œâ”€â”€ useFetch.ts
â”‚   â”‚   â””â”€â”€ useLocalStorage.ts
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ api.ts
â”‚   â”‚   â”œâ”€â”€ auth.service.ts
â”‚   â”‚   â””â”€â”€ user.service.ts
â”‚   â”œâ”€â”€ store/
â”‚   â”‚   â”œâ”€â”€ index.ts
â”‚   â”‚   â””â”€â”€ slices/
â”‚   â”‚       â”œâ”€â”€ authSlice.ts
â”‚   â”‚       â””â”€â”€ userSlice.ts
â”‚   â”œâ”€â”€ types/
â”‚   â”‚   â””â”€â”€ index.ts
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ formatters.ts
â”‚   â”‚   â””â”€â”€ validators.ts
â”‚   â”œâ”€â”€ App.tsx
â”‚   â””â”€â”€ main.tsx
â”œâ”€â”€ public/
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â”œâ”€â”€ vite.config.ts
â””â”€â”€ README.md""",
        "queries": [
            "Find the authentication hook",
            "Where is the data table component?",
            "Find API service configuration",
        ]
    },
    {
        "name": "python_ml_project",
        "description": "Python æœºå™¨å­¦ä¹ é¡¹ç›®",
        "structure": """ml-pipeline/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ loader.py
â”‚   â”‚   â”œâ”€â”€ preprocessor.py
â”‚   â”‚   â””â”€â”€ augmentation.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_model.py
â”‚   â”‚   â”œâ”€â”€ transformer.py
â”‚   â”‚   â””â”€â”€ cnn.py
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ trainer.py
â”‚   â”‚   â”œâ”€â”€ callbacks.py
â”‚   â”‚   â””â”€â”€ metrics.py
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ evaluator.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py
â”‚       â””â”€â”€ logger.py
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_eda.ipynb
â”‚   â””â”€â”€ 02_experiments.ipynb
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ model_config.yaml
â”‚   â””â”€â”€ train_config.yaml
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ [+5 files (5 py) & 0 dirs]
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”œâ”€â”€ outputs/
â”‚   â””â”€â”€ checkpoints/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md""",
        "queries": [
            "Find the model training loop",
            "Where is data preprocessing implemented?",
            "Find the learning rate scheduler",
        ]
    }
]

# å·¥å…·å®šä¹‰ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
TOOLS = [
    {
        "name": "grep",
        "description": "Search for a pattern in files",
        "parameters": {
            "type": "object",
            "properties": {
                "pattern": {"type": "string", "description": "Search pattern (regex)"},
                "path": {"type": "string", "description": "Directory or file to search"},
                "include": {"type": "string", "description": "File pattern to include"}
            },
            "required": ["pattern", "path"]
        }
    },
    {
        "name": "read_file",
        "description": "Read contents of a file",
        "parameters": {
            "type": "object",
            "properties": {
                "path": {"type": "string", "description": "Path to the file"},
                "start_line": {"type": "integer", "description": "Start line number"},
                "end_line": {"type": "integer", "description": "End line number"}
            },
            "required": ["path"]
        }
    },
    {
        "name": "list_dir",
        "description": "List files in a directory",
        "parameters": {
            "type": "object",
            "properties": {
                "path": {"type": "string", "description": "Directory path"}
            },
            "required": ["path"]
        }
    }
]


def build_prompt(project: dict, query: str, max_tree_lines: int = 50) -> str:
    """
    æ„é€ æµ‹è¯• prompt
    
    Args:
        project: é¡¹ç›®ä¿¡æ¯
        query: ç”¨æˆ·æŸ¥è¯¢
        max_tree_lines: æ–‡ä»¶æ ‘æœ€å¤§è¡Œæ•°ï¼ˆæˆªæ–­é€»è¾‘ï¼‰
    """
    tools_str = json.dumps(TOOLS, ensure_ascii=False, indent=2)
    
    # æ–‡ä»¶æ ‘æˆªæ–­é€»è¾‘
    tree_lines = project['structure'].strip().split('\n')
    if len(tree_lines) > max_tree_lines:
        tree = '\n'.join(tree_lines[:max_tree_lines])
        tree += f"\n... (truncated, {len(tree_lines) - max_tree_lines} more items)"
    else:
        tree = project['structure']
    
    # JSON Schema ç¤ºä¾‹ - è®©æ¨¡å‹çŸ¥é“ç²¾ç¡®çš„è¾“å‡ºæ ¼å¼
    output_schema = '''
Output format - you MUST use this exact JSON structure:
<tool_calls>
[{"name": "tool_name", "arguments": {"param1": "value1", "param2": "value2"}}]
</tool_calls>

Example:
<tool_calls>
[{"name": "grep", "arguments": {"pattern": "def authenticate", "path": "app/models/"}}]
</tool_calls>
'''
    
    prompt = f"""<|im_start|>system
You are a code search agent. You help users find code in repositories.

Available tools:
{tools_str}

{output_schema}
<|im_end|>
<|im_start|>user
Project structure:
{tree}

Query: {query}
<|im_end|>
<|im_start|>assistant
"""
    return prompt


def validate_tool_call(tool_call: dict, project: dict) -> dict:
    """
    éªŒè¯å·¥å…·è°ƒç”¨çš„åˆç†æ€§
    
    Returns:
        dict: {"valid": bool, "issues": list, "score": float}
    """
    issues = []
    score = 1.0
    
    name = tool_call.get("name", "")
    args = tool_call.get("arguments", {})
    
    # å¤„ç† arguments æ˜¯å­—ç¬¦ä¸²çš„æƒ…å†µ
    if isinstance(args, str):
        try:
            args = json.loads(args)
        except:
            issues.append("arguments ä¸æ˜¯æœ‰æ•ˆçš„ JSON")
            return {"valid": False, "issues": issues, "score": 0.3}
    
    # 1. æ£€æŸ¥å·¥å…·åç§°
    valid_tools = ["grep", "read_file", "list_dir"]
    if name not in valid_tools:
        issues.append(f"æœªçŸ¥å·¥å…·: {name}")
        score -= 0.5
    
    # 2. æ£€æŸ¥å¿…éœ€å‚æ•°
    if name == "grep":
        if "pattern" not in args:
            issues.append("grep ç¼ºå°‘ pattern å‚æ•°")
            score -= 0.3
        if "path" not in args:
            issues.append("grep ç¼ºå°‘ path å‚æ•°")
            score -= 0.2
        # æ£€æŸ¥ pattern æ˜¯å¦ä¸ºç©º
        if args.get("pattern", "") == "":
            issues.append("pattern ä¸ºç©º")
            score -= 0.3
            
    elif name == "read_file":
        if "path" not in args:
            issues.append("read_file ç¼ºå°‘ path å‚æ•°")
            score -= 0.5
            
    elif name == "list_dir":
        if "path" not in args:
            issues.append("list_dir ç¼ºå°‘ path å‚æ•°")
            score -= 0.3
    
    # 3. æ£€æŸ¥è·¯å¾„æ˜¯å¦åœ¨é¡¹ç›®ç»“æ„ä¸­ï¼ˆç²—ç•¥æ£€æŸ¥ï¼‰
    if "path" in args:
        path = args["path"]
        structure = project.get("structure", "")
        # æ£€æŸ¥è·¯å¾„çš„æŸéƒ¨åˆ†æ˜¯å¦å‡ºç°åœ¨ç»“æ„ä¸­
        path_parts = path.replace("\\", "/").split("/")
        found = any(part in structure for part in path_parts if part and part != ".")
        if not found and path not in [".", "./", "/"]:
            issues.append(f"è·¯å¾„ '{path}' å¯èƒ½ä¸å­˜åœ¨äºé¡¹ç›®ä¸­")
            score -= 0.1  # è½»å¾®æ‰£åˆ†ï¼Œå› ä¸ºå¯èƒ½æ˜¯æ¨¡ç³Šæœç´¢
    
    return {
        "valid": len(issues) == 0,
        "issues": issues,
        "score": max(0, score)
    }


def test_model(model_path: str, base_model_name: str = "Qwen/Qwen3-1.7B"):
    """æµ‹è¯• SFT æ¨¡å‹"""
    print("=" * 60)
    print("SFT æ¨¡å‹æµ‹è¯•")
    print("=" * 60)
    
    # åŠ è½½æ¨¡å‹
    print("\n[1/3] åŠ è½½æ¨¡å‹...")
    print(f"  Base: {base_model_name}")
    print(f"  LoRA: {model_path}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True,
    )
    
    model = PeftModel.from_pretrained(base_model, model_path)
    model.eval()
    
    print("  âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # æµ‹è¯•
    print("\n[2/3] å¼€å§‹æµ‹è¯•...")
    results = []
    
    for project in MOCK_PROJECTS:
        print(f"\nğŸ“ é¡¹ç›®: {project['name']} ({project['description']})")
        
        for query in project['queries']:
            print(f"\n  ğŸ” Query: {query}")
            
            prompt = build_prompt(project, query)
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=512,  # å¢åŠ ä»¥é¿å…æˆªæ–­
                    temperature=0.3,
                    do_sample=True,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                )
            
            response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=False)
            
            # æ£€æŸ¥è¾“å‡ºæ ¼å¼
            has_tool_calls = "<tool_calls>" in response
            valid_json = False
            tool_name = None
            parsed_call = None
            validation = {"valid": False, "issues": ["æœªè§£æ"], "score": 0}
            
            if has_tool_calls:
                try:
                    start = response.find("<tool_calls>") + len("<tool_calls>")
                    end = response.find("</tool_calls>")
                    if end > start:
                        json_str = response[start:end].strip()
                        parsed = json.loads(json_str)
                        valid_json = True
                        if isinstance(parsed, list) and len(parsed) > 0:
                            parsed_call = parsed[0]
                            tool_name = parsed_call.get("name", "unknown")
                            # å¤„ç† arguments æ˜¯å­—ç¬¦ä¸²çš„æƒ…å†µ
                            args = parsed_call.get("arguments", {})
                            if isinstance(args, str):
                                try:
                                    parsed_call["arguments"] = json.loads(args)
                                except:
                                    pass  # ä¿æŒåŸæ ·
                            # éªŒè¯å·¥å…·è°ƒç”¨åˆç†æ€§
                            validation = validate_tool_call(parsed_call, project)
                    elif end == -1 and start > 0:
                        # æ²¡æœ‰ </tool_calls> ç»“æŸæ ‡ç­¾ï¼Œå¯èƒ½è¢«æˆªæ–­
                        validation = {"valid": False, "issues": ["è¾“å‡ºè¢«æˆªæ–­ï¼Œç¼ºå°‘ </tool_calls>"], "score": 0.2}
                except json.JSONDecodeError as e:
                    # å°è¯•ä¿®å¤å¸¸è§çš„ JSON é”™è¯¯
                    error_msg = str(e)
                    if "Expecting ',' delimiter" in error_msg or "Unterminated string" in error_msg:
                        validation = {"valid": False, "issues": ["JSON è¢«æˆªæ–­æˆ–æ ¼å¼é”™è¯¯"], "score": 0.1}
                    else:
                        validation = {"valid": False, "issues": [f"JSON è§£æé”™è¯¯: {error_msg[:50]}"], "score": 0}
                except Exception as e:
                    validation = {"valid": False, "issues": [f"è§£æå¼‚å¸¸: {str(e)[:50]}"], "score": 0}
            
            # ç»¼åˆè¯„åˆ†
            is_success = has_tool_calls and valid_json and validation["valid"]
            status = "âœ…" if is_success else ("âš ï¸" if valid_json else "âŒ")
            
            print(f"     {status} tool_calls: {has_tool_calls}, valid_json: {valid_json}, tool: {tool_name}")
            print(f"     éªŒè¯åˆ†æ•°: {validation['score']:.2f}, é—®é¢˜: {validation['issues'] if validation['issues'] else 'æ— '}")
            
            # æ‰“å°éƒ¨åˆ†è¾“å‡º
            short_response = response[:200].replace('\n', ' ')
            print(f"     Response: {short_response}...")
            
            results.append({
                "project": project['name'],
                "query": query,
                "has_tool_calls": has_tool_calls,
                "valid_json": valid_json,
                "tool_name": tool_name,
                "parsed_call": parsed_call,
                "validation": validation,
                "response": response,
                "prompt": prompt  # ä¿å­˜å®Œæ•´ prompt ç”¨äºåˆ†æ
            })
    
    # ç»Ÿè®¡
    print("\n" + "=" * 60)
    print("[3/3] æµ‹è¯•ç»“æœç»Ÿè®¡")
    print("=" * 60)
    
    total = len(results)
    has_tool_calls_count = sum(1 for r in results if r['has_tool_calls'])
    valid_json_count = sum(1 for r in results if r['valid_json'])
    fully_valid_count = sum(1 for r in results if r['validation']['valid'])
    avg_validation_score = sum(r['validation']['score'] for r in results) / total if total > 0 else 0
    
    print(f"\næ€»æµ‹è¯•: {total}")
    print(f"æœ‰ tool_calls: {has_tool_calls_count}/{total} ({100*has_tool_calls_count/total:.1f}%)")
    print(f"JSON æœ‰æ•ˆ: {valid_json_count}/{total} ({100*valid_json_count/total:.1f}%)")
    print(f"å®Œå…¨åˆç†: {fully_valid_count}/{total} ({100*fully_valid_count/total:.1f}%)")
    print(f"å¹³å‡éªŒè¯åˆ†æ•°: {avg_validation_score:.2f}")
    
    # å·¥å…·ä½¿ç”¨åˆ†å¸ƒ
    tool_counts = {}
    for r in results:
        if r['tool_name']:
            tool_counts[r['tool_name']] = tool_counts.get(r['tool_name'], 0) + 1
    
    if tool_counts:
        print(f"\nå·¥å…·ä½¿ç”¨åˆ†å¸ƒ:")
        for tool, count in sorted(tool_counts.items(), key=lambda x: -x[1]):
            print(f"  - {tool}: {count}")
    
    # å¸¸è§é—®é¢˜ç»Ÿè®¡
    all_issues = []
    for r in results:
        all_issues.extend(r['validation']['issues'])
    if all_issues:
        issue_counts = {}
        for issue in all_issues:
            # ç®€åŒ–é—®é¢˜æè¿°
            key = issue.split(":")[0] if ":" in issue else issue
            issue_counts[key] = issue_counts.get(key, 0) + 1
        print(f"\nå¸¸è§é—®é¢˜:")
        for issue, count in sorted(issue_counts.items(), key=lambda x: -x[1])[:5]:
            print(f"  - {issue}: {count}æ¬¡")
    
    # è¯„çº§ - åŸºäºå®Œå…¨åˆç†çš„æ¯”ä¾‹
    score = fully_valid_count / total * 100
    if score >= 80:
        grade = "ä¼˜ç§€ ğŸŒŸ"
    elif score >= 60:
        grade = "è‰¯å¥½ ğŸ‘"
    elif score >= 40:
        grade = "åŠæ ¼ ğŸ˜"
    else:
        grade = "éœ€æ”¹è¿› âš ï¸"
    
    print(f"\nç»¼åˆè¯„åˆ†: {score:.1f}% - {grade}")
    
    # ä¿å­˜è¯¦ç»†ç»“æœåˆ°æ–‡ä»¶
    save_results(results, model_path)
    
    return results


def save_results(results: list, model_path: str):
    """ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶ï¼Œæ–¹ä¾¿åˆ†ææ¨¡å‹è¾“å‡ºçš„é€»è¾‘æ€§"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ç¡®å®šè¾“å‡ºç›®å½•
    output_dir = os.path.join(os.path.dirname(model_path), "test_results")
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. ä¿å­˜ JSON æ ¼å¼ï¼ˆæœºå™¨å¯è¯»ï¼‰
    json_path = os.path.join(output_dir, f"sft_test_{timestamp}.json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ“„ JSON ç»“æœä¿å­˜åˆ°: {json_path}")
    
    # 2. ä¿å­˜å¯è¯»æŠ¥å‘Šï¼ˆäººå·¥åˆ†æï¼‰
    report_path = os.path.join(output_dir, f"sft_test_{timestamp}.md")
    with open(report_path, "w", encoding="utf-8") as f:
        f.write("# SFT æ¨¡å‹æµ‹è¯•æŠ¥å‘Š\n\n")
        f.write(f"- æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"- æ¨¡å‹è·¯å¾„: {model_path}\n")
        f.write(f"- æµ‹è¯•ç”¨ä¾‹æ•°: {len(results)}\n\n")
        
        # ç»Ÿè®¡
        total = len(results)
        success = sum(1 for r in results if r['valid_json'])
        f.write(f"## ğŸ“Š ç»Ÿè®¡\n\n")
        f.write(f"- æˆåŠŸç‡: {success}/{total} ({100*success/total:.1f}%)\n\n")
        
        # è¯¦ç»†ç»“æœ
        f.write("## ğŸ“ è¯¦ç»†æµ‹è¯•ç»“æœ\n\n")
        f.write("æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹éƒ½æ˜¯ **ç‹¬ç«‹çš„**ï¼Œæ— å†å²ä¸Šä¸‹æ–‡ã€‚\n\n")
        
        for i, r in enumerate(results, 1):
            status = "âœ…" if r['valid_json'] else "âŒ"
            f.write(f"---\n\n")
            f.write(f"### æµ‹è¯• {i}: {status}\n\n")
            f.write(f"**é¡¹ç›®**: {r['project']}\n\n")
            f.write(f"**Query**: {r['query']}\n\n")
            f.write(f"**å·¥å…·è°ƒç”¨**: {r['tool_name'] or 'æ— '}\n\n")
            
            # æ¨¡å‹è¾“å‡º
            f.write(f"**æ¨¡å‹è¾“å‡º**:\n\n")
            f.write("```\n")
            f.write(r['response'])
            f.write("\n```\n\n")
            
            # é€»è¾‘åˆ†æ
            f.write(f"**é€»è¾‘åˆ†æ**:\n\n")
            analysis = analyze_response_logic(r)
            for point in analysis:
                f.write(f"- {point}\n")
            f.write("\n")
    
    print(f"ğŸ“„ Markdown æŠ¥å‘Šä¿å­˜åˆ°: {report_path}")
    print(f"\nğŸ’¡ æç¤º: æŸ¥çœ‹æŠ¥å‘Šæ–‡ä»¶å¯åˆ†ææ¨¡å‹è¾“å‡ºçš„é€»è¾‘æ€§")


def analyze_response_logic(result: dict) -> list:
    """åˆ†æå•ä¸ªå“åº”çš„é€»è¾‘æ€§"""
    analysis = []
    response = result['response']
    query = result['query'].lower()
    
    # 1. æ£€æŸ¥æ˜¯å¦æœ‰ tool_calls
    if result['has_tool_calls']:
        analysis.append("âœ… æ­£ç¡®ä½¿ç”¨äº† tool_calls æ ¼å¼")
    else:
        analysis.append("âŒ æ²¡æœ‰ä½¿ç”¨ tool_calls æ ¼å¼")
        return analysis
    
    # 2. æ£€æŸ¥ JSON æ˜¯å¦æœ‰æ•ˆ
    if result['valid_json']:
        analysis.append("âœ… JSON æ ¼å¼æ­£ç¡®")
    else:
        analysis.append("âŒ JSON æ ¼å¼é”™è¯¯")
        return analysis
    
    # 3. æ£€æŸ¥å·¥å…·é€‰æ‹©æ˜¯å¦åˆç†
    tool = result['tool_name']
    if tool == 'grep':
        analysis.append("âœ… é€‰æ‹© grep å·¥å…· - é€‚åˆæœç´¢ä»£ç æ¨¡å¼")
    elif tool == 'list_dir':
        analysis.append("âš ï¸ é€‰æ‹© list_dir å·¥å…· - é€‚åˆæµè§ˆç›®å½•ç»“æ„")
    elif tool == 'read_file':
        analysis.append("âš ï¸ é€‰æ‹© read_file å·¥å…· - é€šå¸¸éœ€è¦å…ˆæœç´¢å†è¯»å–")
    else:
        analysis.append(f"â“ ä½¿ç”¨äº†æœªçŸ¥å·¥å…·: {tool}")
    
    # 4. æ£€æŸ¥æœç´¢æ¨¡å¼æ˜¯å¦ä¸ query ç›¸å…³
    try:
        start = response.find("<tool_calls>") + len("<tool_calls>")
        end = response.find("</tool_calls>")
        json_str = response[start:end].strip()
        parsed = json.loads(json_str)
        
        if isinstance(parsed, list) and len(parsed) > 0:
            args = parsed[0].get("arguments", {})
            
            # æ£€æŸ¥ grep pattern
            if "pattern" in args:
                pattern = args["pattern"].lower()
                # æ£€æŸ¥ pattern æ˜¯å¦ä¸ query ç›¸å…³
                keywords = extract_keywords(query)
                matches = [k for k in keywords if k in pattern]
                if matches:
                    analysis.append(f"âœ… æœç´¢æ¨¡å¼ä¸æŸ¥è¯¢ç›¸å…³: '{args['pattern']}' åŒ…å«å…³é”®è¯ {matches}")
                else:
                    analysis.append(f"âš ï¸ æœç´¢æ¨¡å¼å¯èƒ½ä¸å¤Ÿç²¾ç¡®: '{args['pattern']}'")
            
            # æ£€æŸ¥è·¯å¾„æ˜¯å¦åˆç†
            if "path" in args:
                path = args["path"]
                if path in [".", "./", "/"]:
                    analysis.append(f"âš ï¸ æœç´¢è·¯å¾„è¾ƒå®½æ³›: '{path}'")
                else:
                    analysis.append(f"âœ… æŒ‡å®šäº†æœç´¢è·¯å¾„: '{path}'")
    except:
        pass
    
    return analysis


def extract_keywords(query: str) -> list:
    """ä» query ä¸­æå–å…³é”®è¯"""
    # å…³é”®è¯æ˜ å°„
    keyword_map = {
        "authentication": ["auth", "authenticate", "login", "user"],
        "login": ["login", "auth", "signin"],
        "email": ["email", "mail", "send"],
        "payment": ["payment", "pay", "transaction", "charge"],
        "jwt": ["jwt", "token", "auth"],
        "order": ["order", "create", "purchase"],
        "hook": ["hook", "use"],
        "table": ["table", "data", "grid"],
        "api": ["api", "service", "fetch", "request"],
    }
    
    keywords = []
    for key, variants in keyword_map.items():
        if key in query:
            keywords.extend(variants)
    
    return list(set(keywords))


def compare_with_base(model_path: str, base_model_name: str = "Qwen/Qwen3-1.7B"):
    """å¯¹æ¯” SFT æ¨¡å‹å’ŒåŸå§‹æ¨¡å‹"""
    print("\n" + "=" * 60)
    print("å¯¹æ¯”æµ‹è¯•ï¼šSFT vs åŸå§‹æ¨¡å‹")
    print("=" * 60)
    
    # ç®€åŒ–æµ‹è¯•
    project = MOCK_PROJECTS[0]
    query = project['queries'][0]
    prompt = build_prompt(project, query)
    
    tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)
    
    # æµ‹è¯•åŸå§‹æ¨¡å‹
    print("\n[åŸå§‹æ¨¡å‹]")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True,
    )
    
    inputs = tokenizer(prompt, return_tensors="pt").to(base_model.device)
    with torch.no_grad():
        outputs = base_model.generate(**inputs, max_new_tokens=256, temperature=0.3, do_sample=True)
    base_response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=False)
    print(f"Response: {base_response[:300]}...")
    
    # æ¸…ç†æ˜¾å­˜
    del base_model
    torch.cuda.empty_cache()
    
    # æµ‹è¯• SFT æ¨¡å‹
    print("\n[SFT æ¨¡å‹]")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True,
    )
    sft_model = PeftModel.from_pretrained(base_model, model_path)
    
    inputs = tokenizer(prompt, return_tensors="pt").to(sft_model.device)
    with torch.no_grad():
        outputs = sft_model.generate(**inputs, max_new_tokens=256, temperature=0.3, do_sample=True)
    sft_response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=False)
    print(f"Response: {sft_response[:300]}...")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="æµ‹è¯• SFT æ¨¡å‹æ•ˆæœ")
    parser.add_argument("--model_path", type=str, required=True, help="SFT æ¨¡å‹è·¯å¾„")
    parser.add_argument("--base_model", type=str, default="Qwen/Qwen3-1.7B", help="åŸºåº§æ¨¡å‹")
    parser.add_argument("--compare", action="store_true", help="æ˜¯å¦ä¸åŸå§‹æ¨¡å‹å¯¹æ¯”")
    
    args = parser.parse_args()
    
    # è¿è¡Œæµ‹è¯•
    results = test_model(args.model_path, args.base_model)
    
    if args.compare:
        compare_with_base(args.model_path, args.base_model)
